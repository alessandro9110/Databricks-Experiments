{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fcb85e5-b780-4187-8feb-54f67645c9ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fine-Tuning a Transformer Model on Azure Databricks\n",
    "\n",
    "## Overview\n",
    "This notebook guides you through the process of preparing data and executing the fine-tuning of a transformer model using Azure Databricks.\n",
    "\n",
    "## Objectives\n",
    "- Setup the environments\n",
    "- Load datasets \n",
    "- Configure and train a transformer model.\n",
    "- Evaluate model performance and save results.\n",
    "\n",
    "\n",
    "## Author\n",
    "- Name: Alessandro Armillotta\n",
    "- Date: 09/10/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97551e36-1d8a-4b90-a38f-b1ccfadfdae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Steps\n",
    "1. Data loading and preprocessing.\n",
    "2. Model configuration and fine-tuning.\n",
    "3. Model evaluation and saving.\n",
    "4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e821ec4d-67d9-4aad-86d5-5858b41f6b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"base_model\", \"google-bert/bert-base-uncased\")\n",
    "base_model = dbutils.widgets.get(\"base_model\")\n",
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f82ca560-2772-42e9-9d21-f5f451ea2234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 0: Setup Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2caa09d8-fadd-4d17-a46b-f48af2c1ee99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, pipeline\n",
    "from pyspark.sql import functions as F\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if device.type == \"cpu\":\n",
    "  no_cuda=True\n",
    "else:\n",
    "  no_cuda=False\n",
    "\n",
    "\n",
    "\n",
    "print(device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc584d7-a53b-4004-a93f-19c51ee66f88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Variables"
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = '/fine_tuning_transformer_model'\n",
    "\n",
    "#artifact_location = \"/Volumes/main/fine_tuning_transformer_model/tmp/artifact\"\n",
    "train_cache_dir   = \"/Volumes/main/fine_tuning_transformer_model/tmp/train\"\n",
    "val_cache_dir     = \"/Volumes/main/fine_tuning_transformer_model/tmp/val\"\n",
    "\n",
    "model_output_dir    = \"/Volumes/main/fine_tuning_transformer_model/tmp/output_model\"\n",
    "model_artifact_path = \"classification\"\n",
    "training_output_dir = \"/Volumes/main/fine_tuning_transformer_model/tmp/trainer\"\n",
    "pipeline_output_dir = \"/Volumes/main/fine_tuning_transformer_model/tmp/pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84456717-21ba-49c8-a0f3-716f4f6fc3ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Experiment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    \n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(\n",
    "            name=experiment_name,\n",
    "            tags={'exp_name': experiment_name}\n",
    "        )\n",
    "        mlflow.set_experiment(experiment_id=experiment_id)\n",
    "        print(f\"Experiment {experiment_name} created.\")\n",
    "    else:\n",
    "        mlflow.set_experiment(experiment_id=experiment.experiment_id)\n",
    "        print(f\"Experiment {experiment_name} already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b7e3ccf-75ac-49e7-948e-a4204d8db832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5c74fd-0143-4ae9-9059-0e89042e4a0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Delta Tables"
    }
   },
   "outputs": [],
   "source": [
    "# load delta tables into dataframe\n",
    "train_df = spark.read.table(\"main.fine_tuning_transformer_model.train_data\")\n",
    "print(train_df.count())\n",
    "val_df   = spark.read.table(\"main.fine_tuning_transformer_model.val_data\")\n",
    "print(val_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175fb107-d44c-44f5-b718-bfe714cb3f42",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get labels"
    }
   },
   "outputs": [],
   "source": [
    "labels = spark.read.table(\"main.fine_tuning_transformer_model.labels\")\n",
    "labels = labels.collect()\n",
    "\n",
    "id2label = {index: row.label for (index, row) in enumerate(labels)}\n",
    "label2id = {row.label: index for (index, row) in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7036ae2f-59a0-4c40-a4d5-445d824d86af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark Dataframe to Transformer Dataset"
    }
   },
   "outputs": [],
   "source": [
    "# drop toPandas() if GPU is not Available \n",
    "train_dataset = train_df.select(\"text\",\"label_id\").withColumn(\"label\", F.col(\"label_id\")).drop(\"label_id\")\n",
    "val_dataset   = val_df.select(\"text\",\"label_id\").withColumn(\"label\", F.col(\"label_id\")).drop(\"label_id\")\n",
    "\n",
    "train_dataset = datasets.Dataset.from_spark(train_dataset, cache_dir=train_cache_dir)\n",
    "val_dataset   = datasets.Dataset.from_spark(val_dataset, cache_dir=val_cache_dir)\n",
    "#train_dataset = datasets.Dataset.from_pandas(train_dataset)\n",
    "#val_dataset   = datasets.Dataset.from_pandas(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed982c29-90ec-4aca-bfb9-8360bc4bf5fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676e6a83-668c-4f4d-abb0-d42b6542cbdb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Tokenizer"
    }
   },
   "outputs": [],
   "source": [
    "# Load Tokenizer baseed on the Model Name. Transformers models expect tokenized input\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"],truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized   = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf27b992-37e9-491d-b750-599d200e0ead",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "AutoModel for Text Classification"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        num_labels=len(label2id),\n",
    "        label2id=label2id,\n",
    "        id2label=id2label\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4e80542-64a1-44b2-b76c-e421f2ab4792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Training arguments refer to a set of hyperparameters that control how a model is trained. These are passed to the TrainingArguments class and used by the Trainer API to manage the training loop. [huggingface.co]\n",
    "\n",
    "Think of training arguments as the recipe for model training: each parameter influences how the model learns, how fast it converges, and how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b622d46e-36a6-4320-875c-fdc2fdd2cc8f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Training Arguments"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=training_output_dir\n",
    "                                 ,per_device_train_batch_size = 50\n",
    "                                 ,per_device_eval_batch_size = 50\n",
    "                                 ,logging_steps = 1\n",
    "                                 ,logging_strategy=\"steps\"\n",
    "                                 ,num_train_epochs = 5\n",
    "                                 ,load_best_model_at_end = True # RECOMMENDED\n",
    "                                 ,metric_for_best_model = \"eval_accuracy\" # RECOMMENDED\n",
    "                                 ,greater_is_better = True\n",
    "                                 ,evaluation_strategy = \"epoch\" # RECOMMENDED\n",
    "                                 ,save_strategy='epoch'\n",
    "                                 ,report_to=\"mlflow\"\n",
    "                                 ,no_cuda=no_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bded42e4-bc61-46c6-9e73-e8b64839e30e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Collator"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b061366-fd21-4b7d-af8a-e4ea4813e83f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Metrics"
    }
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015ec242-0a5a-42d3-b2d2-745b1da3c1a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Trainer"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f680a6df-eec1-44d2-8527-29a88b0c0fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Training and MLFlow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c495e8f-99c2-42d1-95e1-ef10ba239d57",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Start Training"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  with mlflow.start_run(experiment_id=experiment.experiment_id, run_name=f\"train_{base_model}\") as run:\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_output_dir)\n",
    "\n",
    "    pipe = pipeline(\"text-classification\", model=AutoModelForSequenceClassification.from_pretrained(model_output_dir), batch_size=1, tokenizer=tokenizer)\n",
    "\n",
    "    pipe.save_pretrained(pipeline_output_dir)\n",
    "\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "      transformers_model=pipe,\n",
    "      artifact_path=model_artifact_path,\n",
    "      input_example=\"Hi there!\")\n",
    "  \n",
    "    #mlflow.end_run()\n",
    "  \n",
    "except Exception as e:\n",
    "  print(f\"An error occurred: {e}\")\n",
    "  mlflow.end_run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb76278b-1f97-4693-8392-704ba03fd290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Load Logged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718ae715-0475-453a-9e6b-799646efc0fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Model"
    }
   },
   "outputs": [],
   "source": [
    "logged_model = f\"runs:/{run.info.run_id}/{model_artifact_path}\"\n",
    "\n",
    "# Load model as a Spark UDF. Override result_type if the model does not return double values.\n",
    "classification_class = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model, result_type='string')\n",
    "\n",
    "test = val_df.limit(50).select(val_df.text, val_df.label).select(val_df.text, val_df.label, classification_class(val_df.text).alias(\"prediction\"))\n",
    "display(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c08a72-c92d-4de0-9137-2b76d6872f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test.write.saveAsTable(\"main.fine_tuning_transformer_model.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a703627-3bfe-4237-a682-20c299fcaa74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1da87313-1e33-4cf2-a78d-f7274b36f06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81ec8134-3fb2-43a8-9e03-43a24e2637d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r '/Workspace/Users/alessandro.armillotta@mitavanadeitaly.onmicrosoft.com/Databricks-Experiments/Model Fine Tuning/requirements.txt'"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02.fine_tuning_with_trainer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

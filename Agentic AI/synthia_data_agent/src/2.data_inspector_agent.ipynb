{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1917afb-c6ca-4539-9ff6-66cfa02cac9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Develop AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132a1001-f3cd-417a-a374-ec6f9d34962a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install libraries"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqq mlflow-skinny[databricks] databricks-langchain databricks-agents uv langgraph-supervisor==0.0.29\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63b6216-e1f1-4514-8712-7ec4835ada5f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define packages"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml\n",
    "from databricks_langchain import  ChatDatabricks, DatabricksFunctionClient, UCFunctionToolkit, set_uc_function_client\n",
    "\n",
    "from pprint import pprint\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import MessagesState \n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Custom imports\n",
    "from configs import variables\n",
    "from prompts import data_inspector\n",
    "\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated, Dict, List, Optional\n",
    "from langgraph.graph.message import add_messages, AnyMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da839f23-f197-43f2-9b2b-417d3d5a99cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "get functions from catalog"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define client to get all the functions into Unity Catalog\n",
    "client = DatabricksFunctionClient()\n",
    "functions = client.list_functions(catalog=variables.CATALOG_NAME, schema=variables.SCHEMA_NAME)\n",
    "func_names = []\n",
    "for f in functions:\n",
    "    func_names.append(f\"{variables.CATALOG_NAME}.{variables.SCHEMA_NAME}.{f.name}\")\n",
    "\n",
    "# assign function to UCFunctionToolkit\n",
    "toolkit = UCFunctionToolkit(function_names=func_names)\n",
    "tools_uc = toolkit.tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e89e4a-cca7-45e8-a4e8-4f0a1dd20d92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "State"
    }
   },
   "outputs": [],
   "source": [
    "# Define planner state\n",
    "class DataInspectorState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7679caf-4b53-477d-8577-66e11ea68069",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Chat Model"
    }
   },
   "outputs": [],
   "source": [
    "## Define which LLM endpoint to use\n",
    "chat_model = ChatDatabricks(endpoint=variables.LLM_ENDPOINT_NAME)\n",
    "#chat_model.invoke(\"Ciao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd70e6f-4749-4fd0-bc42-c113479b23cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "planner_agent"
    }
   },
   "outputs": [],
   "source": [
    "# Create system message for Agents\n",
    "prompt = SystemMessage(content=data_inspector.system_prompt)\n",
    "\n",
    "# Create an Agent\n",
    "def data_inspector_agent(state: DataInspectorState):\n",
    "    response = [chat_model.invoke([prompt] + state[\"messages\"])]\n",
    "    last_message = response[-1]\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fee7409-8e2c-42f7-93a9-8336ec9b704d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "graph"
    }
   },
   "outputs": [],
   "source": [
    "# Build graph\n",
    "builder = StateGraph(DataInspectorState)\n",
    "builder.add_node(\"data_inspector\", data_inspector_agent)\n",
    "\n",
    "# Logic\n",
    "builder.add_edge(START, \"data_inspector\")\n",
    "builder.add_node(\"tools\", ToolNode(tools_uc\n",
    "                                   ))\n",
    "\n",
    "builder.add_edge(\"data_inspector\", \"tools\")\n",
    "builder.add_conditional_edges(\n",
    "    \"data_inspector\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"data_inspector\")\n",
    "\n",
    "\n",
    "# Add memory. In this way we can save the state of the agent and mantain the memory to save the chat history. \n",
    "# This can be extended with external memory with Lakebase\n",
    "memory = MemorySaver()\n",
    "react_graph = builder.compile()\n",
    "\n",
    "\n",
    "# Show the graph\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc72fea6-461e-49b5-8112-8937424bf20e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test agent"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "messages = [HumanMessage(content=\"\"\"\n",
    "  {\n",
    "    \"domain\": \"retail\",\n",
    "    \"num_records\": 1000,\n",
    "    \"uc_catalog_source\": \"financial\",\n",
    "    \"uc_schema_source\": \"sales\",\n",
    "    \"uc_table_source\": \"sales,\n",
    "    \"uc_catalog_target\": \"financial\",\n",
    "    \"uc_schema_target\": \"sales\",\n",
    "    \"uc_table_target\": \"sales\"\n",
    "\n",
    "  }\"\"\")]\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"alessandro2\"} }\n",
    "\n",
    "messages = react_graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab002ca-e5fe-4c99-82c1-3e9be4f7d63c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3bdf781-fb28-4f5e-854d-c5eff6b337b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mosaic AI Agent wrap  an agent into ResponseAgent\n",
    "How to author an LangGraph agent and wrap it using the ResponsesAgent interface to make it compatible with Mosaic AI. [Response Agent](https://mlflow.org/docs/latest/genai/serving/responses-agent/)\n",
    "\n",
    "The ResponsesAgent extends MLflow's PyFunc model interface to support conversational AI applications that require advanced capabilities such as multi-turn dialogue, tool-calling, multi-agent orchestration, and compatibility with OpenAI's Responses API and MLflow model tracking.\\\n",
    "\n",
    "You can use this notebook [Notebook](https://docs.databricks.com/aws/en/notebooks/source/generative-ai/responses-agent-langgraph.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e003e1-7eea-43af-9fdd-1a93d9e72e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create and wrap the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c3f157-cffd-48e8-9c4f-302b835aacc6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wrap the agent"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agents/planner_agent.py\n",
    "import yaml\n",
    "from databricks_langchain import  ChatDatabricks\n",
    "from typing import Any, Generator, Literal\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "\n",
    "import mlflow\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "\n",
    "from configs import variables\n",
    "from prompts import planner\n",
    "\n",
    "\n",
    "# Define LLm endpoint\n",
    "chat_model = ChatDatabricks(endpoint=variables.LLM_ENDPOINT_NAME)\n",
    "\n",
    "# Set State\n",
    "class PlannerState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "# Create system message for Agents\n",
    "prompt = SystemMessage(content=planner.system_prompt)\n",
    "\n",
    "# Create an Agent\n",
    "def planner_agent(state: PlannerState):\n",
    "    response = [chat_model.invoke([prompt] + state[\"messages\"])]\n",
    "    last_message = response[-1]\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def _langchain_to_responses(self, message: BaseMessage) -> list[dict[str, Any]]:\n",
    "        \"Convert from ChatCompletion dict to Responses output item dictionaries. Ignore user and human messages\"\n",
    "        message = message.model_dump()\n",
    "        role = message[\"type\"]\n",
    "        output = []\n",
    "        if role == \"ai\":\n",
    "            if message.get(\"content\"):\n",
    "                output.append(\n",
    "                    self.create_text_output_item(\n",
    "                        text=message[\"content\"],\n",
    "                        id=message.get(\"id\") or str(uuid4()),\n",
    "                    )\n",
    "                )\n",
    "            if tool_calls := message.get(\"tool_calls\"):\n",
    "                output.extend(\n",
    "                    [\n",
    "                        self.create_function_call_item(\n",
    "                            id=message.get(\"id\") or str(uuid4()),\n",
    "                            call_id=tool_call[\"id\"],\n",
    "                            name=tool_call[\"name\"],\n",
    "                            arguments=json.dumps(tool_call[\"args\"]),\n",
    "                        )\n",
    "                        for tool_call in tool_calls\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        elif role == \"tool\":\n",
    "            output.append(\n",
    "                self.create_function_call_output_item(\n",
    "                    call_id=message[\"tool_call_id\"],\n",
    "                    output=message[\"content\"],\n",
    "                )\n",
    "            )\n",
    "        elif role == \"user\" or \"human\":\n",
    "            pass\n",
    "        return output\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(self, request: ResponsesAgentRequest,) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = self.prep_msgs_for_cc_llm([i.model_dump() for i in request.input])\n",
    "        first_name = True\n",
    "        seen_ids = set()\n",
    "\n",
    "        for event_name, events in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\"]):\n",
    "            if event_name == \"updates\":\n",
    "                if not first_name:\n",
    "                    node_name = tuple(events.keys())[0]  # assumes one name per node\n",
    "                    yield ResponsesAgentStreamEvent(\n",
    "                        type=\"response.output_item.done\",\n",
    "                        item=self.create_text_output_item(\n",
    "                            text=f\"<name>{node_name}</name>\",\n",
    "                            id=str(uuid4()),\n",
    "                        ),\n",
    "                    )\n",
    "                for node_data in events.values():\n",
    "                    for msg in node_data[\"messages\"]:\n",
    "                        if msg.id not in seen_ids:\n",
    "                            print(msg.id, msg)\n",
    "                            seen_ids.add(msg.id)\n",
    "                            for item in self._langchain_to_responses(msg):\n",
    "                                yield ResponsesAgentStreamEvent(\n",
    "                                    type=\"response.output_item.done\", item=item\n",
    "                                )\n",
    "            first_name = False\n",
    "\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(PlannerState)\n",
    "builder.add_node(\"planner\", planner_agent)\n",
    "\n",
    "# Logic\n",
    "builder.add_edge(START, \"planner\")\n",
    "\n",
    "\n",
    "# Add memory. In this way we can save the state of the agent and mantain the memory to save the chat history. \n",
    "# This can be extended with external memory with Lakebase\n",
    "memory = MemorySaver()\n",
    "react_graph = builder.compile()\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "AGENT = LangGraphResponsesAgent(react_graph)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0e0799a-ed9d-4025-ae1f-b66a373758f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d154e0f-1f16-440f-b644-19c370c90c98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Restart Python"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb32d071-33a1-4e49-b862-a85332dfa6d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test the Agent"
    }
   },
   "outputs": [],
   "source": [
    "from agents.planner_agent import AGENT\n",
    "from configs import variables\n",
    "\n",
    "# TODO: Replace this placeholder `input_example` with a domain-specific prompt for your agent.\n",
    "input_example = {\"input\": [{\"role\": \"user\", \"content\": \"Hi what can I do with you?\"}]}\n",
    "\n",
    "AGENT.predict(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a318cb7-973d-4f07-86d1-d3444662c087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Log Agent as an MLFLow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41dba04-ff95-42d7-9fd4-98c00ae30cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from configs import variables\n",
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"planner_agent\",\n",
    "        python_model=\"agents/planner_agent.py\",\n",
    "        code_paths=[\"configs\", \"prompts\"],\n",
    "        pip_requirements=[\n",
    "            \"databricks-langchain\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "        ],\n",
    "        resources= [DatabricksServingEndpoint(endpoint_name=variables.LLM_ENDPOINT_NAME)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ece7b51-33fd-45c6-b358-fe87a08e9532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pre Deployment Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57cc0844-1a09-422b-b14c-a4ece2547ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/planner_agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"Hi what can I do with you?\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d29a39fb-a85d-4590-866e-32ed59c6055e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Register the model to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fff672e-ea5f-41d9-ba63-cc6a021df18b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "UC_MODEL_NAME = f\"{variables.CATALOG_NAME}.{variables.SCHEMA_NAME}.planner_agent\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f47acefe-9569-48f7-8392-2950edcc279f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Deploy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f76507-36cd-4c1e-a29c-f6535e5b916a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME,\n",
    "    uc_registered_model_info.version,\n",
    "    tags={\"endpointSource\": \"docs\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca2316e1-e9a6-4f39-9eac-b14be4476ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.data_inspector_agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
